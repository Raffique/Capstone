{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Dropout,Conv2D,MaxPooling2D, BatchNormalization, LSTM\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.preprocessing import image\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras_preprocessing.image.dataframe_iterator import DataFrameIterator\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:33:29.322202Z","iopub.execute_input":"2022-05-11T21:33:29.322774Z","iopub.status.idle":"2022-05-11T21:33:36.935538Z","shell.execute_reply.started":"2022-05-11T21:33:29.32267Z","shell.execute_reply":"2022-05-11T21:33:36.934668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nsrc = '/kaggle/input'\n\ndf1 = pd.read_csv(\"/kaggle/input/csv-creator-1/train.csv\")\ndf1['image'] = 'csv-creator-1/' + df1['StudyInstanceUID'] + '-' + df1['SeriesInstanceUID'] + '-' + df1['SOPInstanceUID'] + '.png'\n\ndf2 = pd.read_csv(\"/kaggle/input/csv-creator-2/train.csv\")\ndf2['image'] = 'csv-creator-2/' + df2['StudyInstanceUID'] + '-' + df2['SeriesInstanceUID'] + '-' + df2['SOPInstanceUID'] + '.png'\n\ndf3 = pd.read_csv(\"/kaggle/input/csv-creator-3/train.csv\")\ndf3['image'] = 'csv-creator-3/' + df3['StudyInstanceUID'] + '-' + df3['SeriesInstanceUID'] + '-' + df3['SOPInstanceUID'] + '.png'\n\ndf4 = pd.read_csv(\"/kaggle/input/csv-creator-4/train.csv\")\ndf4['image'] = 'csv-creator-4/' + df4['StudyInstanceUID'] + '-' + df4['SeriesInstanceUID'] + '-' + df4['SOPInstanceUID'] + '.png'\n\ndf5 = pd.read_csv(\"/kaggle/input/csv-creator-5/train.csv\")\ndf5['image'] = 'csv-creator-5/' + df5['StudyInstanceUID'] + '-' + df5['SeriesInstanceUID'] + '-' + df5['SOPInstanceUID'] + '.png'\n\ndf6 = pd.read_csv(\"/kaggle/input/csv-creator-6/train.csv\")\ndf6['image'] = 'csv-creator-6/' + df6['StudyInstanceUID'] + '-' + df6['SeriesInstanceUID'] + '-' + df6['SOPInstanceUID'] + '.png'\n\ndf7 = pd.read_csv(\"/kaggle/input/csv-creator-7/train.csv\")\ndf7['image'] = 'csv-creator-7/' + df7['StudyInstanceUID'] + '-' + df7['SeriesInstanceUID'] + '-' + df7['SOPInstanceUID'] + '.png'\n\ndf = pd.concat([df1,df2,df3,df4,df5,df6,df7], ignore_index=True)#df = pd.read_csv(\"/kaggle/input/newcsv/new_train.csv\")\n\n#only rows needed are those with PE\n#files with pe equal => 96540\n\nx = 'image' #independent variables for DCMDataFrameIterator\ny = ['pe_present_on_image'] ##dependent variables for DCMDataFrameIterator\nclass_mode = 'raw'\n\n# split for testing\ntrain_df, test_df = train_test_split(df, test_size=0.1)\n\n# augmentation parameters\n# you can use preprocessing_function instead of rescale in all generators\n# if you are using a pretrained network\ntrain_augmentation_parameters = dict(\n    rescale=1.0/255.0,\n    rotation_range=10,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest',\n    brightness_range = [0.8, 1.2],\n    validation_split = 0.2\n)\n\nvalid_augmentation_parameters = dict(\n    rescale=1.0/255.0,\n    validation_split = 0.2\n)\n\ntest_augmentation_parameters = dict(\n    rescale=1.0/255.0\n)\n\n#<<==============================================================================================================>>\n# training parameters\nBATCH_SIZE = 32\nCLASS_MODE = 'raw'\nCOLOR_MODE = 'rgb'\nTARGET_SIZE = (512, 512)\nEPOCHS = 10\nSEED = 1337\n\ntrain_consts = {\n    'seed': SEED,\n    'batch_size': BATCH_SIZE,\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE,  \n    'subset': 'training',\n    'validate_filenames':False\n}\n\nvalid_consts = {\n    'seed': SEED,\n    'batch_size': BATCH_SIZE,\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE, \n    'subset': 'validation',\n    'validate_filenames':False\n}\n\ntest_consts = {\n    'batch_size': 32,  # should be 1 in testing\n    'class_mode': CLASS_MODE,\n    'color_mode': COLOR_MODE,\n    'target_size': TARGET_SIZE,  # resize input images\n    'shuffle': False,\n    'validate_filenames':False\n}\n\n#<<=================================================================================================================================================>>\n# Using the training phase generators \ntrain_augmenter = ImageDataGenerator(**train_augmentation_parameters)\nvalid_augmenter = ImageDataGenerator(**valid_augmentation_parameters)\ntest_augmenter = ImageDataGenerator(**test_augmentation_parameters)\n\n\ntrain_generator = train_augmenter.flow_from_dataframe(dataframe = train_df, directory=src, x_col=x, y_col=y, **train_consts)\n\n\n\nvalid_generator = valid_augmenter.flow_from_dataframe(dataframe = train_df, directory=src, x_col=x, y_col=y, **valid_consts)\n\ntest_generator = test_augmenter.flow_from_dataframe(dataframe = test_df, directory=src, x_col=x, y_col=y, **test_consts)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-11T21:33:39.917178Z","iopub.execute_input":"2022-05-11T21:33:39.917459Z","iopub.status.idle":"2022-05-11T21:33:40.316012Z","shell.execute_reply.started":"2022-05-11T21:33:39.91743Z","shell.execute_reply":"2022-05-11T21:33:40.315001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#<<=================================================================================================================================================>>\n# define model architecture like how you normally do\n#https://www.analyticsvidhya.com/blog/2021/12/cnn-model-covid-19/\n\n\nmodel=Sequential()\n\n#covolution layer\nmodel.add(Conv2D(32,(3,3),activation='relu',input_shape=[512,512,3]))\n#pooling layer\nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\n\n#covolution layer\nmodel.add(Conv2D(32,(3,3),activation='relu'))\n#pooling layer\nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\n\n#covolution layer\nmodel.add(Conv2D(64,(3,3),activation='relu'))\n#pooling layer\nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\n\n#covolution layer\nmodel.add(Conv2D(64,(3,3),activation='relu'))\n#pooling layer\nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\n\n#i/p layer\nmodel.add(Flatten())\n\n#o/p layer\nmodel.add(Dense(1,activation='sigmoid'))\n\n#compile network\nopt = SGD(learning_rate=0.01, momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_accuracy'])\n#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())\n#<<=================================================================================================================================================>>\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:33:48.354497Z","iopub.execute_input":"2022-05-11T21:33:48.354805Z","iopub.status.idle":"2022-05-11T21:33:48.603651Z","shell.execute_reply.started":"2022-05-11T21:33:48.354776Z","shell.execute_reply":"2022-05-11T21:33:48.602504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# training\n# define the checkpoint\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfilepath = \"best_model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\n#uncomment for continous training\n#model = load_model('/kaggle/input/pe-results/model.h5')\n\nhistory = model.fit(\ntrain_generator,\nepochs=EPOCHS,\nvalidation_data=valid_generator,\nverbose=1, \ncallbacks=callbacks_list\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-11T21:34:02.930618Z","iopub.execute_input":"2022-05-11T21:34:02.930967Z","iopub.status.idle":"2022-05-11T21:34:44.126817Z","shell.execute_reply.started":"2022-05-11T21:34:02.930931Z","shell.execute_reply":"2022-05-11T21:34:44.124522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#commented out cuz modelcheckpoint previous block takes care of saving model\nmodel.save('end_model.h5')\nmodel.save_weights('weights.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(test_generator, steps=len(test_generator), batch_size=1)\nprint('test loss: {},  test_accuracy: {}'.format(test_loss, test_accuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loss graph\nplt.plot(history.history['loss'],label='train loss')\nplt.plot(history.history['val_loss'],label='val loss')\nplt.legend()\n\nplt.savefig('loss-graph.png')\nplt.show()\n\n# accuracies\nplt.plot(history.history['binary_accuracy'], label='train acc')\nplt.plot(history.history['val_binary_accuracy'], label='val acc')\nplt.legend()\n\n \nplt.savefig('acc-graph.png')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}